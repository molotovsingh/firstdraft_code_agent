Now I have enough context. Let me create the implementation according to the requirements:

*** Begin Patch

```diff
--- a/shared/storage/s3.py
+++ b/shared/storage/s3.py
@@ -50,3 +50,17 @@ class Storage:
         """Return presigned GET URL for temporary download."""
         return self.client.presigned_get_object(self.bucket, key, expires=timedelta(seconds=expiry))
 
+    def copy_object(self, src_key: str, dst_key: str) -> None:
+        """Copy object from src_key to dst_key within the same bucket."""
+        from minio.commonconfig import CopySource
+        copy_source = CopySource(self.bucket, src_key)
+        self.client.copy_object(self.bucket, dst_key, copy_source)
+
+    def remove_object(self, key: str) -> None:
+        """Remove object from bucket."""
+        self.client.remove_object(self.bucket, key)
+
+    def object_exists(self, key: str) -> bool:
+        """Check if object exists."""
+        from minio.error import S3Error
+        try:
+            self.client.stat_object(self.bucket, key)
+            return True
+        except S3Error:
+            return False
```

```diff
--- a/apps/block0_api/main.py
+++ b/apps/block0_api/main.py
@@ -424,3 +424,91 @@ def processed_json(document_id: str):
         }
     finally:
         db.close()
+
+
+@app.post("/v0/uploads/finalize")
+def finalize_upload(payload: dict = Body(...)):
+    """
+    Finalize a presigned upload by moving it to permanent storage and creating DB records.
+    """
+    tenant_id = payload.get("tenant_id")
+    user_id = payload.get("user_id")
+    key = payload.get("key")
+    filename = payload.get("filename")
+    mime = payload.get("mime") or "application/octet-stream"
+
+    if not tenant_id or user_id is None or not key:
+        raise HTTPException(status_code=400, detail="tenant_id, user_id, and key are required")
+
+    db = SessionLocal()
+    storage = Storage()
+    try:
+        # Validate tenant and user
+        tenant = db.get(models.Tenant, uuid.UUID(tenant_id))
+        if tenant is None:
+            raise HTTPException(status_code=400, detail="Unknown tenant_id")
+        user = db.get(models.User, user_id)
+        if user is None or str(user.tenant_id) != tenant_id:
+            raise HTTPException(status_code=400, detail="Invalid user for tenant")
+
+        # Fetch object bytes from staging location
+        try:
+            content = storage.get_object_bytes(key)
+        except Exception as e:
+            log.error("finalize_fetch_error", key=key, error=str(e))
+            raise HTTPException(status_code=404, detail="Object not found at key")
+
+        # Compute SHA256
+        sha256 = Storage.sha256_hex(content)
+
+        # Derive final storage key
+        if not filename:
+            # Extract filename from key if not provided
+            import os
+            filename = os.path.basename(key)
+            # Remove any suffix added during presign
+            if '.' in filename:
+                parts = filename.rsplit('.', 1)
+                if len(parts[1]) == 8 and all(c in '0123456789abcdef' for c in parts[1]):
+                    filename = parts[0]
+
+        final_key = storage.object_key(tenant_id=str(tenant.id), sha256=sha256, version=1, filename=filename)
+
+        # Copy from staging to final location
+        storage.copy_object(key, final_key)
+
+        # Create Document record
+        doc = models.Document(
+            id=uuid.uuid4(),
+            tenant_id=tenant.id,
+            user_id=user.id,
+            case_ref=None,
+            orig_filename=filename,
+            mime=mime,
+            bytes_sha256=sha256,
+        )
+        db.add(doc)
+        db.flush()
+
+        # Create DocumentVersion
+        ver = models.DocumentVersion(
+            document_id=doc.id,
+            version=1,
+            storage_uri=final_key,
+        )
+        db.add(ver)
+        db.flush()
+
+        # Create ProcessingJob
+        job = models.ProcessingJob(
+            id=uuid.uuid4(),
+            document_id=doc.id,
+            status=ProcessingStatus.queued,
+        )
+        db.add(job)
+
+        # Estimate credits
+        estimate = estimate_credits(mime, len(content))
+        credit = models.Credit(
+            tenant_id=tenant.id,
+            user_id=user.id,
+            delta=-estimate,
+            reason="estimate",
+            job_id=job.id,
+            is_estimate=True,
+        )
+        db.add(credit)
+        db.commit()
+
+        # Enqueue for processing
+        enqueue_process_document(str(job.id))
+
+        # Return response
+        return JSONResponse({
+            "document_id": str(doc.id),
+            "job_id": str(job.id),
+            "version": 1,
+            "storage_uri": final_key,
+            "credit_estimate": estimate
+        })
+    finally:
+        db.close()
```

```diff
--- a/API.md
+++ b/API.md
@@ -36,6 +36,38 @@ Response 200:
 }
 ```
 
+## POST /v0/uploads/finalize
+Finalize a presigned upload by moving it to permanent storage and creating database records.
+
+- JSON body:
+  - `tenant_id` (UUID string, required)
+  - `user_id` (int, required)
+  - `key` (string, required): The object_key from presign where file was uploaded
+  - `filename` (string, optional): Override filename, otherwise extracted from key
+  - `mime` (string, optional): MIME type, default `application/octet-stream`
+
+Process:
+1. Fetches object from staging location
+2. Computes SHA256 hash
+3. Copies to final storage location
+4. Creates Document and DocumentVersion records
+5. Creates ProcessingJob and enqueues for processing
+6. Estimates credits
+
+Response 200:
+```json
+{
+  "document_id": "uuid",
+  "job_id": "uuid",
+  "version": 1,
+  "storage_uri": "tenant/ab/abcdef.../v1/filename",
+  "credit_estimate": 42
+}
+```
+
+Response 404 if object not found at key.
+Response 400 for invalid tenant_id/user_id.
+
 ## POST /v0/documents/upload
 - Multipart form-data parameters:
   - `tenant_id` (UUID, required)
```

```diff
--- /dev/null
+++ b/scripts/smoke_finalize.py
@@ -0,0 +1,99 @@
+#!/usr/bin/env python3
+"""
+End-to-end smoke test for presigned upload -> finalize flow.
+"""
+import json
+import time
+import requests
+import sys
+
+API_BASE = "http://localhost:8000"
+
+def main():
+    # Test data
+    tenant_id = "00000000-0000-0000-0000-000000000001"
+    user_id = 1
+    test_content = b"This is a test document for the finalize flow.\nLine 2.\nLine 3.\n"
+    filename = "test_finalize.txt"
+    mime = "text/plain"
+
+    print("=== Presigned Upload + Finalize Flow Test ===\n")
+
+    # Step 1: Get presigned URL
+    print("1. Getting presigned upload URL...")
+    presign_resp = requests.post(f"{API_BASE}/v0/uploads/presign", json={
+        "tenant_id": tenant_id,
+        "user_id": user_id,
+        "filename": filename,
+        "mime": mime
+    })
+    if presign_resp.status_code != 200:
+        print(f"Error getting presigned URL: {presign_resp.status_code}")
+        print(presign_resp.text)
+        sys.exit(1)
+    
+    presign_data = presign_resp.json()
+    upload_url = presign_data["url"]
+    object_key = presign_data["object_key"]
+    print(f"   Got presigned URL for key: {object_key}")
+
+    # Step 2: Upload directly to S3/MinIO
+    print("\n2. Uploading content directly to S3...")
+    upload_resp = requests.put(upload_url, data=test_content, headers={"Content-Type": mime})
+    if upload_resp.status_code not in (200, 204):
+        print(f"Error uploading to S3: {upload_resp.status_code}")
+        print(upload_resp.text)
+        sys.exit(1)
+    print("   Upload successful")
+
+    # Step 3: Finalize the upload
+    print("\n3. Finalizing upload...")
+    finalize_resp = requests.post(f"{API_BASE}/v0/uploads/finalize", json={
+        "tenant_id": tenant_id,
+        "user_id": user_id,
+        "key": object_key,
+        "filename": filename,
+        "mime": mime
+    })
+    if finalize_resp.status_code != 200:
+        print(f"Error finalizing upload: {finalize_resp.status_code}")
+        print(finalize_resp.text)
+        sys.exit(1)
+    
+    finalize_data = finalize_resp.json()
+    document_id = finalize_data["document_id"]
+    job_id = finalize_data["job_id"]
+    storage_uri = finalize_data["storage_uri"]
+    credit_estimate = finalize_data["credit_estimate"]
+    
+    print(f"   Document ID: {document_id}")
+    print(f"   Job ID: {job_id}")
+    print(f"   Storage URI: {storage_uri}")
+    print(f"   Credit estimate: {credit_estimate}")
+
+    # Step 4: Poll job status
+    print("\n4. Polling job status...")
+    for i in range(30):  # Max 30 seconds
+        job_resp = requests.get(f"{API_BASE}/v0/jobs/{job_id}")
+        if job_resp.status_code != 200:
+            print(f"Error getting job status: {job_resp.status_code}")
+            sys.exit(1)
+        
+        job_data = job_resp.json()
+        status = job_data["status"]
+        print(f"   Status: {status}")
+        
+        if status == "completed":
+            break
+        elif status == "failed":
+            print(f"   Job failed: {job_data.get('error')}")
+            sys.exit(1)
+        
+        time.sleep(1)
+
+    # Step 5: Fetch processed document
+    print("\n5. Fetching processed document...")
+    processed_resp = requests.get(f"{API_BASE}/v0/documents/{document_id}/processed.json")
+    if processed_resp.status_code == 200:
+        processed_data = processed_resp.json()
+        print(f"   Version: {processed_data['version']}")
+        print(f"   Metrics: {json.dumps(processed_data['metrics'], indent=2)}")
+        print(f"   Warnings: {processed_data['warnings']}")
+        print("\nâœ… Test completed successfully!")
+    else:
+        print(f"Error fetching processed document: {processed_resp.status_code}")
+        sys.exit(1)
+
+if __name__ == "__main__":
+    main()
```

*** End Patch

## Reasoning

The implementation adds the finalize-from-presigned flow with minimal changes:

1. **shared/storage/s3.py**: Added `copy_object()` for moving objects from staging to final location, and `remove_object()` for future deletion capability (though not used per requirements).

2. **apps/block0_api/main.py**: Added `/v0/uploads/finalize` endpoint that:
   - Validates tenant/user like existing endpoints
   - Fetches object bytes from staging key
   - Computes SHA256 and derives final storage location
   - Copies object to permanent storage (doesn't delete staging per requirements)
   - Creates Document, DocumentVersion, ProcessingJob, and Credit records
   - Enqueues job for processing
   - Returns standardized response

3. **API.md**: Documented the new endpoint with request/response format matching existing documentation style.

4. **scripts/smoke_finalize.py**: Complete end-to-end test that demonstrates the full flow from presign through processing.

The implementation follows existing patterns from `/v0/documents/upload`, reuses existing helpers, maintains consistency with current error handling, and avoids any schema changes or new dependencies.
