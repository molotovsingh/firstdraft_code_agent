Now I have a comprehensive understanding of the current state. Let me create a detailed phased implementation plan for taking FirstDraft v2.0 Block 0a to production-ready MVP and preparing for Block 1 integration.

# FirstDraft v2.0 Block 0a → Production MVP Implementation Plan
**Engineering-Led 6-8 Week Roadmap**

## Executive Summary

Based on analysis of the current scaffold, we have solid foundation code with FastAPI endpoints, Celery processing, basic DB schema, and Docker infrastructure. The path to production-ready Block 0a requires implementing real OCR capabilities, robust quality assessment, proper error handling, observability, and clean interfaces for Block 1 handoff.

## Phase 1: Core Processing Engine (Weeks 1-3)
**Milestone: Production-quality OCR and document processing pipeline**

### Sprint 1.1: OCR Implementation (Week 1)
**Focus: Real document processing capabilities**

**Tasks:**
1. **OCR Adapter Implementation** (`shared/ocr/`)
   - Replace stub in `shared/quality/metrics.py:50` with real OCR
   - Implement Tesseract adapter with Hindi/English language support
   - Add OCRmyPDF adapter for PDF documents with selectable text
   - Feature-flag paddle/cloud OCR for future cost-effective options
   - **Owner:** Backend developer with OCR experience
   - **AC:** Process PDF, image, and scanned documents; extract text with confidence scores

2. **Image Normalization Pipeline** (`shared/image_processing/`)
   - OpenCV/Pillow-based image enhancement (contrast, brightness, rotation)
   - Skew detection and auto-correction
   - Resolution optimization for OCR quality
   - **Owner:** Computer vision/image processing developer
   - **AC:** Improve OCR accuracy on low-quality images by 25%+

3. **Language Detection and Handling**
   - Hindi/English mixed document processing
   - Language-specific OCR configuration
   - Content language metadata tracking
   - **Owner:** NLP/multilingual processing developer  
   - **AC:** Correctly identify and process Hindi, English, and mixed documents

**Deliverables:**
- Working OCR pipeline replacing stub at `apps/block0_worker/worker.py:50`
- Image enhancement utilities in `shared/image_processing/`
- OCR text stored in MinIO with URIs in `document_versions.ocr_text_uri`

### Sprint 1.2: Quality Assessment Engine (Week 2)
**Focus: Comprehensive document quality metrics and warnings**

**Tasks:**
1. **Page-Level Metrics Implementation** (`shared/quality/metrics.py`)
   - Real DPI detection, blur measurement, skew calculation
   - OCR confidence aggregation per page and document
   - Language mix detection and scoring
   - **AC:** Replace all placeholder metrics with real measurements

2. **Warning Generation System**
   - Threshold-based warning triggers (blur > 0.35, OCR conf < 0.7)
   - Actionable user guidance ("Try sharper scan", "Check image rotation")
   - Quality impact predictions for downstream processing
   - **AC:** Generate specific, helpful warnings that improve user experience

3. **Content-Addressed Storage Layout**
   - Organized object keys: `tenant/{tenant_id}/docs/{sha256}/v{version}/{filename}`
   - Separate storage for original, processed, and OCR text files
   - Deduplication through SHA-256 content addressing
   - **AC:** Efficient storage utilization; no duplicate content stored

**Deliverables:**
- Production-quality metrics in `shared/quality/metrics.py`
- Enhanced warning system with actionable guidance
- Clean storage organization supporting versioning and deduplication

### Sprint 1.3: Pipeline Hardening (Week 3)
**Focus: Robust error handling and job reliability**

**Tasks:**
1. **Idempotent Job Design**
   - Page-level processing chunks for large documents
   - Resume capability for failed jobs
   - Partial success handling (some pages processed, others failed)
   - **AC:** Jobs can be safely retried; partial failures don't lose progress

2. **Enhanced Database Schema** (`shared/db/models.py`)
   - Add indexes on `bytes_sha256`, `tenant_id`, `user_id` 
   - Page-level processing metadata
   - Enhanced job step tracking with timestamps
   - **AC:** Query performance under load; detailed processing audit trail

3. **Credits Logic Refinement**
   - Accurate cost estimation based on document complexity
   - Page-count-based pricing for large documents
   - Budget vs recommended model differentiation
   - **AC:** Cost estimates within 20% of actual; clear model selection impact

**Deliverables:**
- Robust job processing with retry capabilities
- Database performance optimizations
- Accurate credit estimation and tracking

## Phase 2: Production Infrastructure (Weeks 4-5)
**Milestone: Production-ready deployment and operations**

### Sprint 2.1: Observability and Monitoring (Week 4)
**Focus: Production visibility and debugging capabilities**

**Tasks:**
1. **Structured Logging Implementation**
   - Request/job correlation IDs throughout pipeline
   - Structured JSON logs with consistent fields
   - Log aggregation preparation (ELK/Loki compatible)
   - **AC:** End-to-end request tracing; searchable, structured logs

2. **Metrics Collection**
   - Task duration metrics (OCR time, queue depth, error rates)
   - Processing throughput (pages/hour, documents/hour)
   - Quality score distributions and warning patterns
   - **AC:** Real-time visibility into system performance and quality trends

3. **Health Checks and Readiness**
   - API health endpoint (`/health`) checking DB, Redis, MinIO connectivity
   - Worker health monitoring and queue depth alerts
   - Dependency health propagation
   - **AC:** Deployment orchestration can determine service health

**Deliverables:**
- Comprehensive logging with correlation IDs
- Metrics dashboard showing processing performance
- Health check endpoints for orchestration

### Sprint 2.2: Security and Privacy (Week 5)
**Focus: Production-grade security and compliance**

**Tasks:**
1. **Per-Tenant Data Isolation**
   - Database connection routing per tenant
   - Storage bucket organization by tenant
   - Audit logging of cross-tenant access attempts
   - **AC:** Complete data separation; no possibility of tenant data leakage

2. **Secrets Management Implementation** 
   - Transition from `.env` to cloud secrets manager
   - API key rotation capabilities
   - Per-tenant encryption keys for sensitive data
   - **AC:** Production-ready secret handling; rotation without downtime

3. **PII Handling and Data Retention**
   - Document content encryption at rest
   - Configurable retention policies per tenant
   - Secure deletion capabilities
   - **AC:** GDPR/privacy compliance; secure data lifecycle management

**Deliverables:**
- Multi-tenant security implementation
- Production secrets management
- Privacy-compliant data handling

## Phase 3: Integration and Hardening (Weeks 6-8)
**Milestone: Block 1 handoff readiness and production deployment**

### Sprint 3.1: Block 1 Interface Preparation (Week 6)
**Focus: Clean handoff interfaces for entity discovery**

**Tasks:**
1. **Structured Output Schema Design**
   - Standardized document metadata format for Block 1 consumption
   - OCR text with confidence regions and page mapping
   - Quality context for entity discovery confidence adjustment
   - **AC:** Block 1 can consume Block 0a outputs without custom parsing

2. **API Enhancements**
   - Health check endpoint: `/v0/health`
   - Batch processing endpoints for Block 1 integration
   - Presigned URL generation for secure object access
   - **AC:** Clean API contract for Block 1 integration

3. **LLM Service Foundation** (`shared/llm_service/router.py`)
   - Basic implementation of centralized LLM routing per ADR-0014
   - OpenRouter integration with fallback to direct providers
   - Credit tracking integration
   - **AC:** Block 1 can use consistent LLM service; fallback reliability

**Deliverables:**
- Standardized Block 0a → Block 1 interface specification
- Enhanced API endpoints supporting integration use cases
- Basic LLM service ready for Block 1 consumption

### Sprint 3.2: Performance and Scale Testing (Week 7)
**Focus: Production load validation**

**Tasks:**
1. **Performance Baseline Testing**
   - Load testing: 100 concurrent uploads, 1000 pages/hour processing
   - Memory usage profiling under sustained load
   - Database query performance under tenant load
   - **AC:** System handles target load with acceptable latency

2. **Golden Dataset Creation**
   - Curated test documents representing real Indian legal scenarios
   - Expected output validation for regression testing
   - Quality threshold validation with real-world documents
   - **AC:** Regression test suite prevents quality degradation

3. **Integration Testing Suite**
   - End-to-end upload → process → retrieve → Block 1 handoff
   - Error scenario testing (network failures, partial processing)
   - Multi-tenant isolation validation
   - **AC:** Comprehensive test coverage; confidence in production reliability

**Deliverables:**
- Performance benchmarks and load test results
- Golden dataset and regression test suite
- Comprehensive integration test coverage

### Sprint 3.3: Production Deployment (Week 8)
**Focus: Production-ready deployment and go-live**

**Tasks:**
1. **Container and K8s Preparation**
   - Production Docker images with security hardening
   - Kubernetes manifests for scaling and reliability
   - CI/CD pipeline for automated deployments
   - **AC:** One-command deployment to production environment

2. **Production Operations Runbook**
   - Deployment procedures and rollback plans
   - Troubleshooting guides for common issues
   - Monitoring alert thresholds and response procedures
   - **AC:** Operations team can deploy and maintain system independently

3. **Block 1 Handoff Documentation**
   - Technical specification for Block 1 integration
   - Sample code and usage examples
   - SLA commitments and interface contracts
   - **AC:** Block 1 development can proceed with clear requirements

**Deliverables:**
- Production deployment capability
- Operational procedures and documentation
- Block 1 integration specification

## Technical Deep Dive: Key Implementation Details

### OCR Architecture Design
```python
# shared/ocr/adapters.py
class OCRAdapter(ABC):
    @abstractmethod
    def process_document(self, content: bytes, mime: str) -> OCRResult

class TesseractAdapter(OCRAdapter):
    def __init__(self, languages=['eng', 'hin']):
        self.languages = languages
    
    def process_document(self, content: bytes, mime: str) -> OCRResult:
        # Image normalization → Tesseract OCR → confidence scoring
        # Page-level processing for large documents
        # Language detection and appropriate model selection
```

### Quality Metrics Implementation
```python
# shared/quality/metrics.py (enhanced)
class QualityAssessment:
    def analyze_document(self, content: bytes, ocr_result: OCRResult) -> QualityReport:
        return QualityReport(
            dpi=self._calculate_dpi(content),
            blur_score=self._measure_blur(content),
            skew_degrees=self._detect_skew(content),
            ocr_confidence=ocr_result.confidence,
            language_mix=self._detect_languages(ocr_result.text),
            page_metrics=[...],  # Per-page breakdown
            warnings=self._generate_warnings(...)
        )
```

### Database Schema Enhancements
```sql
-- Additional indexes for performance
CREATE INDEX idx_documents_tenant_user ON documents(tenant_id, user_id);
CREATE INDEX idx_documents_sha256 ON documents(bytes_sha256);
CREATE INDEX idx_processing_jobs_status ON processing_jobs(status);
CREATE INDEX idx_credits_tenant_date ON credits(tenant_id, created_at);

-- Page-level processing tracking
ALTER TABLE document_versions ADD COLUMN page_count INTEGER;
ALTER TABLE document_versions ADD COLUMN processing_metadata JSONB;
```

### Content-Addressed Storage Layout
```
MinIO Bucket: firstdraft-{env}
├── tenant/{tenant-uuid}/
│   └── docs/{sha256}/
│       ├── v1/
│       │   ├── original/{filename}
│       │   ├── processed/{filename}.pdf
│       │   └── ocr/text.json
│       └── v2/ (if reprocessed with different settings)
```

## Risk Assessment and Mitigations

### High-Risk Items
1. **OCR Quality on Poor Documents**
   - *Risk:* Indian market document quality may defeat OCR
   - *Mitigation:* Extensive image preprocessing; multiple OCR engines; graceful degradation

2. **Multi-Tenant Data Isolation**
   - *Risk:* Data leakage between tenants
   - *Mitigation:* Database-per-tenant architecture; comprehensive audit logging; security testing

3. **Performance Under Load**
   - *Risk:* System cannot handle target throughput
   - *Mitigation:* Early load testing; horizontal scaling design; performance monitoring

### Medium-Risk Items
1. **Block 1 Integration Complexity**
   - *Risk:* Interface mismatches delay Block 1
   - *Mitigation:* Early interface specification; integration testing; mock Block 1 implementation

2. **Credit System Accuracy**
   - *Risk:* Cost estimates significantly wrong
   - *Mitigation:* Conservative estimates initially; usage data collection; iterative refinement

## Success Criteria and Validation

### Technical Success Metrics
- **Processing Accuracy**: 95%+ OCR accuracy on standard documents; 80%+ on poor quality
- **Performance**: Process 1000 pages/hour; handle 100 concurrent uploads
- **Reliability**: 99.5% job success rate; automatic retry for transient failures
- **Quality**: Generate actionable warnings for 90%+ of low-quality inputs

### Business Success Metrics
- **User Trust**: Lawyer confident enough to use outputs in case preparation
- **Cost Predictability**: Credit estimates within 20% of actual consumption
- **Integration Readiness**: Block 1 team can begin development with stable interfaces
- **Operational Maturity**: System deployable and maintainable in production

## Timeline Summary (Gantt-Style)

```
Weeks 1-3: Core Processing Engine
├─ Week 1: OCR Implementation (Tesseract, image processing)
├─ Week 2: Quality assessment, warning generation  
└─ Week 3: Job reliability, database optimization

Weeks 4-5: Production Infrastructure  
├─ Week 4: Observability (logging, metrics, health checks)
└─ Week 5: Security (multi-tenant isolation, secrets management)

Weeks 6-8: Integration and Hardening
├─ Week 6: Block 1 interface preparation, LLM service foundation
├─ Week 7: Performance testing, golden dataset creation
└─ Week 8: Production deployment, operational procedures
```

## Day 1 Execution Punch List

1. **Set up development environment** - Verify Docker Compose, database connectivity
2. **Implement basic Tesseract OCR adapter** - Replace stub in `shared/quality/metrics.py`
3. **Add image preprocessing pipeline** - Basic contrast/brightness adjustment
4. **Create real quality metrics** - DPI detection, blur measurement
5. **Enhance database indexes** - Add performance indexes identified above
6. **Implement structured logging** - Add correlation IDs, JSON output
7. **Create health check endpoint** - `/v0/health` with dependency checks
8. **Begin golden dataset collection** - Gather representative test documents
9. **Set up basic monitoring** - Metrics collection for processing times
10. **Document Block 1 interface requirements** - Initial specification draft

This plan balances aggressive timeline with quality requirements, emphasizing foundation-first approach while preparing for seamless Block 1 integration. The modular structure allows parallel development once core infrastructure is stable.
