I will start by reading the main files in each of the key directories to understand their functionality and how they interact. This will help me identify potential issues and formulate recommendations.
Here is a concise architectural review:

**Architectural Review & Recommendations**

**1. Hotspots & Risk Areas**
*   **API Upload Bottleneck:** The `/v0/documents/upload` endpoint in `apps/block0_api/main.py` is synchronous, reading full files into memory. This is a performance risk and will fail on large uploads.
*   **Fragile Worker Logic:** The main task in `apps/block0_worker/worker.py` is a monolithic function. A failure in any step (e.g., a `subprocess` call to `ocrmypdf`) fails the entire job, making it hard to debug or retry.
*   **DB Connection Management:** DB sessions are manually created and closed in every API route in `apps/block0_api/main.py`. This is error-prone and can lead to connection leaks.
*   **Configuration Sprawl:** Configuration (e.g., `DATABASE_URL`, S3 details) is scattered, read via `os.getenv` in multiple files (`shared/db/session.py`, `shared/storage/s3.py`, `apps/block0_api/main.py`).

**2. Quick Wins**
*   **Adopt Presigned URLs:** The `presign_upload` endpoint exists. Immediately switch client-side scripts like `scripts/batch_upload.py` to use it. Deprecate the synchronous `upload_documents` endpoint.
*   **Use FastAPI Dependencies for DB:** Refactor `apps/block0_api/main.py` to use FastAPI's `Depends` for managing `SessionLocal`. This will centralize session lifecycle and cleanup.
*   **Centralize Settings:** Use a Pydantic `Settings` class to manage all environment-variable-based configuration.
*   **Improve Health Checks:** The `/healthz` endpoint in `apps/block0_api/main.py` should return a non-200 status code when a component is down to properly signal failure to orchestrators.

**3. Tech Debt to Schedule**
*   **Refactor Worker into a Pipeline:** Break down the `process_document` task in `apps/block0_worker/worker.py` into a chain of smaller, idempotent Celery tasks (e.g., `fetch_file`, `run_ocr`, `compute_metrics`).
*   **Consolidate OCR Logic:** The OCR logic is implemented directly in the worker, while `shared/ocr/adapters/` contains unused stubs. Refactor the worker to use the adapter pattern properly.
*   **Optimize Docker Images:** The `infra/dockerfiles/Dockerfile.worker` is large. Use multi-stage builds to reduce the final image size by separating build-time dependencies from runtime ones.

**4. Missing Tests & Docs**
*   **No Tests:** The project has no automated tests. Add unit tests for business logic (e.g., `shared/quality/metrics.py`) and integration tests for the API-worker flow.
*   **Incomplete Documentation:** `API.md`, `ADR.md`, and `PRD.md` are empty. Document key architectural decisions and the API contract, especially the presigned URL workflow.

**5. Security Pitfalls**
*   **Hardcoded Credentials:** `docker-compose.yml` contains default credentials. These should be removed and managed via environment files or secrets management. The `scripts/bootstrap_dev.py` script also uses a hardcoded dev tenant UUID.
*   **Missing API Authentication:** API endpoints lack authentication/authorization, relying only on a `tenant_id` in the payload. Implement a robust auth mechanism (e.g., JWTs, API keys).
*   **Request ID Logging:** The `RequestIDMiddleware` in `apps/block0_api/main.py` is a good start, but the request ID is not logged with `structlog`, limiting its utility for tracing.
